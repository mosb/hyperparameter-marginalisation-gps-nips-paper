\documentclass{article}
\usepackage{nips12submit_e,times}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption} 
\usepackage{pstool}
\usepackage{psfrag}
\newcommand\jansays[1]{\textcolor{blue}{Jan says: #1}}
\newcommand\mikesays[1]{\colorbox{yellow}{\parbox{\columnwidth}{Mike says: #1}}}

\newcommand{\p}[2]{p\!\left(#1\middle\vert#2\right)}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\xd}{\vect{x}}
\newcommand{\yd}{\vect{y}}

\DeclareMathOperator*{\argmax}{\arg\!\max\!}

\title{Approximate Hyperparameter Marginalisation for Gaussian Processes}

\author{
Rob\\
\And
Steve\\
\And
Jan\\
\And
Mike\\
\And
Steve\\}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version


\begin{document}


\maketitle

%------- ABSTRACT --------
\begin{abstract}
Kernel and covariance functions are often parameterised by hyperparameters, unknown {\it a-priori}. We propose a novel means of approximately integrating over these hyperparameters. This approach gives rise to a novel class of covariance functions which are robust to hyperparameter uncertainty, avoiding the over-fitting that often accompanies type-II maximum-likelihood fitting. We demonstrate the efficacy of our covariances for Gaussian process regression on both synthetic and real data.
\end{abstract}


%------- INTRODUCTION--------
\section{Introduction}

Kernel and covariance functions are vital to much of machine learning and data mining \citep{shawe2004kernel}. In this paper, we consider Gaussian process \citep{rassandwill} inference, which has at its heart just such a covariance function. The hyperparameters that define such functions are rarely known {\it a-priori}. Given training data, these hyperparameters are often set by type-II maximum likelihood, which can lead to overfitting. A more rigorous alternative is to integrate over, or marginalise, the hyperparameters, which is unfortunately usually intractable. While sampling approaches \citep{neal1997monte} have been developed to address this problem, they are prohibitively computationally expensive for large numbers of hyperparameters. 

We introduce a Laplace approach to approximately marginalise covariance hyperparameters. This gives us a robust family of covariances that are tolerant to uncertainty in the hyperparameters. Our method is particularly appropriate when such uncertainty is likely to be significant, as when only small amounts of data are available. We are motivated by scenarios where obtaining data is expensive (computationally or otherwise), such as in global optimisation \citep{osborne2009gaussian}. 


% \jansays{In Bayesian methods a common problem is to choose a prior. In inference with Gaussian processes this task could involve choosing kernel function hyperparameters. In practice however, it is often unclear how to make this choice a priori. Therefore, most implementations deviate from proper Bayesian treatment by estimating the hyperparameters from the data via maximum lmarginal-ikelihood methods. In contrast, we propose to add another hierarchy of inference on top of that. In particular, we propose to place a prior distribution over the hyperparameters. Its hyperparemeters can in turn be estimated learned from the data. Since the resuting integrals of the marginalizations are non-analytic we use a Taylor expansion to yield a Gaussian process which approximates the correct process with marginalized hyperparameters.
% PERHAPS WRITE ABOUT INTEGRAL KERNEL AND RELATION TO RATIONAL QUADRATIC?
% We conduct experiments illustrating the benefits our approach on artificial as well as on real data.}


%------- GAUSSIAN PROCESSES--------
\section{Gaussian Processes}

Gaussian processes ({\scshape gp}s) constitute a powerful method for performing Bayesian inference about functions using a limited set of observations \cite{rassandwill}. A {\scshape gp} is defined as a distribution over the functions $f : \mathcal{X} \rightarrow \mathbb{R}$ such that the distribution over the possible function values on any finite set of $\mathcal{X}$ is multi-variate Gaussian.
%One can imagine a point $x_i \in \mathcal{X}$ as having an associated random variable $Y_i$ representing the possible values the function can take at that point, the {\scshape gp} is therefore the infinite set of such random variables defined across the domain of the function. If we take a sample from a {\scshape gp} at a finite set of input locations $\mathbf{x} = \{x_1,...,x_n\}$, the corresponding observations $\mathbf{y} = \{ y_1,...,y_n\}$ can be viewed as a single point sampled from some multi-variate Gaussian distribution.

A {\scshape gp} is completely defined by its first and second moments: a mean function $\mu : \mathcal{X} \rightarrow \mathbb{R}$, which describes the overall trend of the function, and a positive semidefinite covariance function, or kernel, $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ which describes how function values are correlated as a function of their locations in the domain. Given a function $f : \mathcal{X} \rightarrow \mathbb{R}$ about which we wish to perform inference and a set of input points $\mathbf{x} \subseteq \mathcal{X}$, the Gaussian process prior distribution over the function values $\mathbf{f} = f(\mathbf{x})$ is given by:
\begin{align}
p(\mathbf{f} | \mathbf{x},\bm{\theta},I) &:= \mathcal{N}\left( \mathbf{f};\mu_\theta(\mathbf{x}),K_\theta(\mathbf{x},\mathbf{x}) \right) \nonumber \\
&:= \frac{1}{\sqrt{\mathrm{det} 2 \pi K_{\mathbf{f}}} } \exp \left( - \frac{1}{2} (\mathbf{f}-\mu_{\mathbf{f}})^\top K_\mathbf{f}^{-1} (\mathbf{f}-\mu_{\mathbf{f}})  \right)
\end{align}
where $\bm{\theta}$ is a vector containing any parameters required by $\mu$ and $K$: the \emph{hyperparameters} of the model, $I$. Due to the ubiquity of $I$ we henceforth drop it from explicit representation for notational convenience. %The context, $I$, forms the background knowledge upon which all our probabilities are conditioned. Its ubiquity leads us to henceforth drop it from explicit representation.
There exist a wide variety of mean and covariance functions which can be chosen in order to reflect any prior knowledge available about the function of interest. 
%Note also that we need not place a {\scshape gp} directly on the function, for example a function known to be strictly positive might benefit from a {\scshape gp} over its logarithm.

%Observations
A vector of observations $\mathbf{y} = \{ y_1,...,y_n\}$ could be viewed as a single point sampled from a $n$-variate Gaussian distribution. Once we have observations of the function $(\mathbf{x_s},\yd)$ we can make predictions about the function value $f_*$ at input $x_*$. As exact measurements of the function are often not available we assume a noise model, such that:
\begin{equation}\label{obsnoise}
p(y | f, x, \sigma_n^2) := \mathcal{N}(y; f, \sigma_n^2)
\end{equation} 
which represents i.i.d Gaussian observation noise with variance $\sigma_n^2$. As should be expected, the predictive distribution over $f_*$ is Gaussian:
\begin{equation}\label{posteriorpred}
p(f_* | x_*, \yd,\bm{\theta}) := \mathcal{N} ( f_* ; m_\theta(f_* | x_*, \yd), C_\theta(f_* | x_*, \yd))
\end{equation}
where the posterior mean and covariance are:
\begin{align}
m_\theta(f_* | x_*, \yd) := \mu_\theta(x_*) + K_\theta(x_*,\xd)V^{-1}(\yd - \mu_\theta(x_*))\\
C_\theta(f_* | x_*, \yd) := K_\theta(x_*,x_*) - K_\theta(x_*,\xd)V^{-1}K_\theta(\xd,x_*)
\end{align}
where $V := K_\theta(\xd,\xd) + \sigma_n^2\mathbf{I}$.

\subsection{Marginalising out the Hyperparameters}
%Marginalisation
The previous equations assume that the hyperparameters $\bm{\theta}$ are known; in fact we can rarely be certain about $\bm{\theta}$ \emph{a priori}. This ignorance can be represented by a suitably uninformative prior distribution $p(\bm{\theta})$. Given such a \emph{hyper-prior}, the hyperparameters can be marginalised to calculate the predictive distribution over $f_*$:
\begin{equation}\label{fullmargint}
p(f_* |x_*, \yd) = \frac{\int p(f_* | x_*, \yd,\bm{\theta})p(\yd|\xd,\bm{\theta})p(\bm{\theta})d\bm{\theta}}{\int p(\yd|\xd,\bm{\theta})p(\bm{\theta})d\bm{\theta}}
\end{equation}
Unfortunately, such integrals are generally non-analytic, requiring numerical approximation. Randomized Monte Carlo techniques \cite{chen2000monte} form the most popular approaches to numerical integration, although Bayesian alternatives \cite{bayesquad, osbornebayesquad} also exist. 
These techniques estimate the integral given
the value of the integrand on a set of sample points, usually via a weighted mixture
\begin{equation}\label{sampling}
p(f_* |x_*, \yd) \simeq \sum_i \rho_i p(f_* | x_*, \yd,\bm{\theta}_i)
\end{equation}
for some weight vector $\bm{\rho}$. Unfortunately, the computational expense of evaluating the integrand at sufficient samples to estimate high-dimensional integrals is often prohibitive. %As a consequence, such approaches are rarely used for the marginalisation of {\scshape gp} hyperparameters. 
%\jansays{IS THIS CORRECT? OR, IS IT BECAUSE IT IS HARD  TO SPECIFY A PRIOR AND SINCE PEOPLE ARE LAZY ?}
% you're not going to win people over by calling them lazy

A less computationally demanding alternative is to select only a single sample. Type II maximum likelihood, or maximum marginal likelihood, approximates as
\begin{equation} \label{eq:ML}
p(f_* |x_*, \yd) \simeq p(f_* | x_*, \yd,\bm{\theta}_{\mathrm{ML}})
\end{equation}
%\begin{equation}\label{marglikelihood}
%p(\yd | \xd, \bm{\theta}) = \int p(\yd | \mathbf{f},\xd, \bm{\theta})p(\mathbf{f} | \mathbf{x},\bm{\theta}) d\mathbf{f}
%\end{equation}
where $\bm{\theta}_{\mathrm{ML}}$ is the hyperparameter vector that maximises the marginal likelihood, 
\begin{equation}
\bm{\theta}_{\mathrm{ML}} = \argmax_{\bm{\theta}} \;\; p(\yd|\xd,\bm{\theta})
\end{equation}
As per Figure \ref{fig:ML}, \eqref{eq:ML} is equivalent to approximating the likelihood $p(\yd|\xd,\bm{\theta})$ as the delta function $\delta(\theta - \theta_{\mathrm{ML}})$. This assumption is a poor representation of our true state of ignorance given only low numbers of data, for which the likelihood is typically broad and/or multi-modal. As a consequence, type II maximum likelihood can often lead to over-fitting. Nonetheless, the less onerous computational requirements of this approach have made it ubiquitous throughout machine learning. 

\begin{figure}
	\begin{subfigure}[b]{7cm}
	  \psfragfig[height=3cm, width=7cm]{figures/direct_samples2}
	  \caption{}
	  \label{fig:ML}
	\end{subfigure}
	\begin{subfigure}[b]{7cm}
	  \psfragfig[height=3cm, width=7cm]{figures/slice_samples2}
	  \caption{}
	  \label{fig:MCMC}
	\end{subfigure}
\caption{\subref{fig:ML}) Samples (black dots) obtained by optimising the log-likelihood (grey) using a global optimiser, and in blue, the maximum likelihood approximation of the likelihood surface. \subref{fig:MCMC}) Samples obtained by taking draws from the posterior using an {\scshape mcmc} method.}
\label{fig:periodic_likelihood}
\end{figure}

We now present a computationally inexpensive approach to approximately marginalising hyperparameters.

%------- APPROXIMATE HYPERPARAMETER MARGINALISATION --------
\section{Approximate Hyperparameter Marginalisation}

We now propose a general-purpose family of covariance functions that result from approximately marginalising the hyperparameters of an homogenous exponential covariance. 

% In this section we introduce an approximate method for performing inference, using a Gaussian process, which accommodates the inherent uncertainty one should have about the value of hyperparameters, but which does not suffer from excessive computational expense. Rather than attempt to approximate the integral in (\ref{fullmargint}), we present a novel approach which uses a modified prior that incorporates and marginalises the uncertainty in the hyperparameters \emph{before} it is fused with observations. The result is an adjusted covariance function which can be used interchangeably with a standard covariance function in {\scshape gp} inference, but whose parameters are now \emph{hyper-hyperparameters} explaining the marginalised covariance function.

\subsection{Homogenous Exponential Covariance Functions} 

We consider the general form of covariance, which we term the {\it homogenous exponential}
\begin{equation}\label{eq:he}
K_\theta(x,x^\prime) = h^2 \exp \left( -\frac{1}{2} \frac{\bigtriangleup^2}{L^2} \right)\,,
\end{equation}
where $h$ is a scaling parameter termed the output scale, and $L$ is the \emph{length scale}. This form captures many of the most commonly used covariance functions. If $\bigtriangleup^2 = \sum_i (x_i - x_i^\prime)^2$, \eqref{eq:he} becomes the ubquitous \emph{squared exponential}. If $\bigtriangleup^2 = |x-x'|$,  \eqref{eq:he} is the Ornstein-Uhlenbeck covariance. Many other covariances can be constructed by taking $\bigtriangleup^2$ as other metrics, allowing for periodic covariances, warpings of the input space \citep{snelson2004warped} and inference in more structured domains \citep{garnett2010bayesian}.

The output scale $h$ can be tractably marginalised according to suitable priors \citep{kennedy1998bayesian}. Even if performing type-II maximum likelihood, the output scale is usually well-specified by even a moderate amount of data. We will not consider this hyperparameter further. 

The length scale $L$ affects how closely outputs co-vary as a function of $\bigtriangleup^2$. Our predictions will ultimately be very sensitive to the value of the length scale; a short length scale will favour rough functions, a long length scale will favour smoother functions. Unfortunately, the likelihood of the length scale is often highly multi-modal: Figure \ref{fig:periodic_likelihood} illustrates the likelihood surface as a function of $\theta = L$ for a periodic covariance, where the multiple modes correspond to harmonics of the latent period and sampling frequency. That is, given limited data, the posterior for the input scale is likely to have large variance. As such, type-II maximum likelihood for this hyperparameter will lead to problematic over-fitting. 

\subsection{Rational Quadratic Covariance Function} 

The Rational Quadratic covariance function represents one means of approximately marginalising the input scale of a squared exponential covariance. It assumes a gamma prior, which, problematically, puts excessive weight on very large input scales when we are very uncertain. See Figure \ref{fig:rq_vs_our_cov}.

\begin{figure}
	\begin{subfigure}[b]{7cm}
		\psfragfig[height = 2cm, width = 7cm]{figures/gamma_prior}
		\caption{Gamma prior distributions for $1/L^2$}
		\label{fig:gamma_prior}
	\end{subfigure}
	\begin{subfigure}[b]{7cm}
		\psfragfig[height = 2cm, width = 7cm]{figures/log_normal_prior}
		\caption{Gaussian prior distributions for $\log(L)$}
		 \label{fig:log_normal_prior}
	\end{subfigure}
 \\[0.2cm]
	\begin{subfigure}[b]{7cm}
		\psfragfig[height = 2cm, width = 7cm]{figures/RQ_cov}
		\caption{Rational quadratic covariances}
		\label{fig:RQ_cov}
	\end{subfigure}
	\begin{subfigure}[b]{7cm}
		\psfragfig[height = 2cm, width = 7cm]{figures/our_cov}
		\caption{Marginal homogenous exponential covariances}
		 \label{fig:our_cov}
	\end{subfigure}
\caption{The different priors assumed by {\scshape rq} and {\scshape mhe} covariances give rise to different behaviours as the variance in the input scale ($\Lambda$) increases. The priors of \subref{fig:gamma_prior} assign more weight to very large input scales, whereas the priors of \subref{fig:log_normal_prior} assign more weight to small input scales. All figures have $\nu = 1$ and share the legends of \subref{fig:gamma_prior}) and \subref{fig:our_cov}).}
\label{fig:rq_vs_our_cov}
\end{figure}


\subsection{Marginal Homogenous Exponential Covariance Function} \mikesays{now we talk about our covariance, which I've called Marginal Homogenous Exponential covariances}


The standard {\scshape gp} prior assumes that the value of $L$ is deterministic; any distribution an individual has over $L$ is purely the epistemic characterisation of their uncertainty in the `true' value of $L$. We alter the prior such that we make $L$ a random variable with its own generative distribution $p(L)$. The value of $L$ should be strictly positive, %\jansays{no the way you defined it above L can be anything. Maybe should def $\exp(-\Delta^2 / L)$ instead of .../$L^2$.. this was the way we used to do it in February}
hence we make the substitution $\beta := \log(L)$ and define a Gaussian distribution over $\beta$ so that $p(L)$ will have strictly positive support: 
\begin{equation}\label{betaSEkernel}
K_\theta(x,x^\prime) = h^2 \exp \left( -\frac{1}{2}\bigtriangleup^2 \exp(-2\beta) \right)
\end{equation}
which we will write as $K_\beta$, and
\begin{align}
p(\beta|\nu,\Lambda) &= \mathcal{N}(\beta; \nu, \Lambda) \nonumber\\
&= \frac{1}{\sqrt{2 \pi \Lambda}} \exp \left( \frac{ -(\beta - \nu)^2}{2\Lambda} \right) \label{betapdf}
\end{align}
where $\nu$ and $\Lambda$ are hyper-hyperparmeters.

As stated before, we marginalise out the uncertainty in $\beta$ (or $L$), before fusing the prior with observations. 

\jansays{Perhaps we should talk about the moment matching idea first.. As far as I can remember: Ie taylor expand the marginalized process ... it then turns out the the kernel of the matched process equals the marginalized kernel you write down in (14) below..}

The marginalised kernel is calculated by evaluating the following integral:
\begin{equation} \label{kernelmargint}
K_{\nu,\Lambda} = \int_{-\infty}^{+\infty} K_\beta \; p(\beta|\nu,\Lambda) d\beta
\end{equation}
which is non-analytic given the current form of $K_\beta$. We approximate the solution to the integral by first approximating $K_\beta$ using a second-order Taylor expansion around the mean of $p(\beta)$ given by $\nu$. In order to ensure that the approximate kernel is positive semi-definite we force $K_\beta$ to be in exponential form, hence we note the following identity and substitution:
\begin{align}
K_\beta &= \exp \left( -\ln \frac{1}{K_\beta} \right) \nonumber\\ 
&= \exp \left( -A \right) \label{Asub}
\end{align}
and we Taylor expand around $A$:
\begin{equation} \label{taylorexpansion}
A \approx \ln \frac{1}{K_{\beta}}\bigg|_{\nu} + (\beta-\nu) \frac{\partial{A}}{\partial{\beta}}\bigg|_{\nu} + \frac{1}{2}(\beta-\nu)^2 \frac{\partial^2{A}}{\partial^2{\beta}}\bigg|_{\nu}
\end{equation}
Given (\ref{betaSEkernel}), the derivatives of $K_\beta$ can be calculated:
\begin{align*}
K^\prime_\beta &= K_\beta \left( \bigtriangleup^2 \exp(-2\beta)  \right) \\ 
K^{\prime\prime}_\beta &= K_\beta \left( \bigtriangleup^4 \exp(-4\beta) -  \bigtriangleup^2 \exp(-2\beta) \right)
\end{align*}
which can be used for calculating the partial derivatives of (\ref{taylorexpansion}):
\begin{align}
A &= \ln \frac{1}{K_\beta} \label{partial1}\\
\frac{\partial{A}}{\partial{\beta}} &= -\frac{K^\prime_\beta}{K_\beta} = \; -\bigtriangleup^2 \exp(-2\beta) \label{partial2}\\
\frac{\partial^2{A}}{\partial^2{\beta}} &= -\frac{K^{\prime\prime}_\beta}{K_\beta} + \frac{{K^\prime_\beta}^2}{K_\beta^2} = \; 2 \bigtriangleup^2 \exp(-2\beta) \label{partial3}
\end{align}
Using (\ref{Asub}) and substituting (\ref{partial1}-\ref{partial3}) into (\ref{taylorexpansion}) we arrive at the following expression for $K_\beta$:
\begin{align}
K_\beta &\approx \exp \left(-\left( \ln \frac{1}{K_{\nu}} - (\beta-\nu) C + (\beta-\nu)^2 C \right)   \right) \nonumber\\
&= K_{\nu} \, \exp \left( (\beta-\nu) C - (\beta-\nu)^2 C \right) \label{approxkernel}
\end{align}
where $C = \bigtriangleup^2 \exp(-2\nu)$

All that remains is to evaluate the integral in (\ref{kernelmargint}), which by using (\ref{betapdf}) and (\ref{approxkernel}) is as follows:
\begin{align}
K_{\nu,\Lambda} &\approx K_{\nu}\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi \Lambda}} \exp \left( \frac{ -(\beta - \nu)^2}{2\Lambda} \right) \left( (\beta-\nu) C - (\beta-\nu)^2 C \right) \nonumber \\
&= K_{\nu} \; \exp \left( \frac{\Lambda\, C^2}{2(1+2\Lambda C)} \right) \frac{1}{\sqrt{1+ 2\Lambda C}}
\end{align}
This result is a new kernel which is the product of the original kernel $K_\beta$ evaluated at $\beta=\nu$, and an expression which is a function of the squared distance $ \bigtriangleup^2$. 



\subsubsection{Proof of Positive Semi-Definiteness}

In order to confirm that our new kernel $K_{\nu,\Lambda}$ is a legitimate covariance function, we must prove that it fulfils the necessary condition of kernels which is positive semi-definiteness. We first note that a sum of kernels is also a kernel, therefore:
\begin{equation}
K_{\nu,\Lambda} = \int K_\beta \;p(\beta)d\beta
\end{equation}
should be a legitimate kernel if $K_\beta$ is also a legitimate kernel as $p(\beta)$ just weights the contents of the integral. Using the expression for $K_\beta$ in (\ref{approxkernel}) and completing the square we arrive at the following expression:
\begin{align}
K_\beta &= K_{\nu} \,  \exp \left(  -\frac{1}{2} \left( \beta - \nu - \frac{1}{2} \right) ^ 2C   \right) \exp \left( \frac{1}{4}C \right)
\end{align}
If we substitute in $K_\nu$, where:
\begin{align}
K_{\nu} &= h^2 \exp\left( -\frac{1}{2}\bigtriangleup^2 \exp(-2\nu) \right) \nonumber\\
&= h^2 \exp\left( -\frac{C}{2} \right)
\end{align}
we can write:
\begin{align}
%K_{\beta} &= h^2 \exp\left( -\frac{C}{2} \right) \,  \exp \left(  -\frac{1}{2} \left( \beta - \nu - \frac{1}{2} \right) ^ 2C   \right) \exp \left( \frac{1}{4}C \right) \nonumber \\
K_{\beta} = h^2  \exp\left( -\frac{C}{4} \right) \,  \exp \left(  -\frac{1}{2} \left( \beta - \nu - \frac{1}{2} \right) ^ 2C   \right) \label{kexpanded}
\end{align}
We note that the product of kernels is also a kernel. Both exponentials in (\ref{kexpanded}) form legitimate kernels, which leaves us with the result that $K_\beta$ is also a kernel and by definition is positive semi-definite as required.






%------- EXPERIMENTS --------
\section{Experiments}



\subsection{Posterior functions with differing length scales}

When using standard kernels, functions drawn from the prior will have a fixed length scale; when using our kernel, the length scale of functions drawn from the prior can take a number a range of values as a result of the distribution over length scales. When our prior is fused with observations to form a posterior {\scshape gp}, we are able to describe functions that may have generated the observations over a range of length scales. When data is limited this is particularly crucial as a whole range of length scales may be permissible with regards to the data generated, even if one is favoured through type-II maximum likelihood methods. As data increases the range of permissible length scales reduces, eventually collapsing to the true length scale.

We demonstrate the above behaviour and its advantage over other methods in this section. We begin by drawing a function from a prior {\scshape gp} with known length scale. From this we can generate a training set of between $X$ and $X$ observations in the domain $\mathcal{X}\in [-3,3]$. We train the hyperparameters or hyper-hyperparameters of our test methods using type-II maximum likelihood on the given training set. Now we go back to the original {\scshape gp} and draw a set of $X$ functions from the posterior (so that the functions still go through the original training data) with length scales ranging from $X$ to $X$. An example training set and resulting set of posterior functions can be seen in figure XXXX. A test set with $100$ observations is generated from each function and a log likelihood is calculated for each method. This process is repeated with $100$ different starting functions and training data.

We plot the mean and variance trials of the likelihood of the test data across all $X*100$ against the log length scale of the posterior function.




\subsection{Real world dataset}










%------- RELATED WORK --------
%\section{Related Work} 
% \mjikesays{no need for this particularly, I don't think; we can cover related work above}




%------- CONCLUSION --------
\section{Conclusion}



%---------ACKNOWLEDGEMENTS-----------
% \subsubsection*{Acknowledgments}
% Do we have any? Aladdin / Orchid?
% no acknowledgements in double-blind submission


%------------REFERENCES----------------
\subsubsection*{References}
\renewcommand{\refname}{\vskip -0.75cm}  %Removes the "References" title
\bibliographystyle{plain}
\small{
\bibliography{Hypermarginalisation}
}




\end{document}






