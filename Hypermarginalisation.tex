\documentclass{article}
\usepackage{nips12submit_e,times}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xcolor}
\newcommand\jansays[1]{\textcolor{blue}{Jan says: #1}}

\newcommand\mikesays[1]{\colorbox{yellow}{\parbox{\columnwidth}{Mike says: #1}}}

\newcommand{\xd}{\mathbf{x}}
\newcommand{\yd}{\mathbf{y}}

\DeclareMathOperator*{\argmax}{\arg\!\max\!}

\title{Approximate Hyperparameter Marginalisation for Gaussian Processes}

\author{
Rob\\
\And
Steve\\
\And
Jan\\
\And
Mike\\
\And
Steve\\}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version


\begin{document}


\maketitle

%------- ABSTRACT --------
\begin{abstract}
Kernel and covariance functions are often parameterised by hyperparameters, unknown {\it a-priori}. We propose a novel means of approximately integrating over these hyperparameters. This approach gives rise to a novel class of covariance functions which are robust to hyperparameter uncertainty, avoiding the over-fitting that often accompanies type-II maximum-likelihood fitting of the hyperparameters. We demonstrate the efficacy of our covariances for Gaussian process regression on both synthetic and real data.
% \jansays{In Bayesian methods a common problem is to choose a prior. In inference with Gaussian processes this task could involve choosing kernel function hyperparameters. In practice however, it is often unclear how to make this choice a priori. Therefore, most implementations deviate from proper Bayesian treatment by estimating the hyperparameters from the data via maximum lmarginal-ikelihood methods. In contrast, we propose to add another hierarchy of inference on top of that. In particular, we propose to place a prior distribution over the hyperparameters. Its hyperparemeters can in turn be estimated learned from the data. Since the resuting integrals of the marginalizations are non-analytic we use a Taylor expansion to yield a Gaussian process which approximates the correct process with marginalized hyperparameters.
% PERHAPS WRITE ABOUT INTEGRAL KERNEL AND RELATION TO RATIONAL QUADRATIC?
% We conduct experiments illustrating the benefits our approach on artificial as well as on real data.}
\end{abstract}


%------- INTRODUCTION--------
\section{Introduction}

Kernels and covariances are vital to much of Machine Learning. 

The hyperparameters of such functions are rarely known {\it a-priori}. This is particularly important when we have small amounts of data, important for applications where obtaining data is expensive (computationally or otherwise).

Hyperparameters are often set by type-II maximum likelihood. Monte Carlo is too expensive. We introduce a Laplace approximation. This gives us a robust family of covariances that are tolerant to uncertainty in the hyperparameters. 

%------- GAUSSIAN PROCESSES--------
\section{Gaussian Processes}

Gaussian processes (\verb"GPs") constitute a powerful method for performing Bayesian inference about functions using a limited set of observations \cite{rassandwill}. A \verb"GP" is defined as a distribution over the functions $f : \mathcal{X} \rightarrow \mathbb{R}$ such that the distribution over the possible function values on any finite set of $\mathcal{X}$ is multi-variate Gaussian. A vector of observations $\mathbf{y} = \{ y_1,...,y_n\}$ could be viewed as a single point sampled from a $n$-variate Gaussian distribution.
%One can imagine a point $x_i \in \mathcal{X}$ as having an associated random variable $Y_i$ representing the possible values the function can take at that point, the \verb"GP" is therefore the infinite set of such random variables defined across the domain of the function. If we take a sample from a \verb"GP" at a finite set of input locations $\mathbf{x} = \{x_1,...,x_n\}$, the corresponding observations $\mathbf{y} = \{ y_1,...,y_n\}$ can be viewed as a single point sampled from some multi-variate Gaussian distribution.

A \verb"GP" is completely defined by its first and second moments: a mean function $\mu : \mathcal{X} \rightarrow \mathbb{R}$, which describes the overall trend of the function, and a positive semidefinite covariance function, or kernel, $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ which describes how function values are correlated as a function of their locations in the domain. Given a function $f : \mathcal{X} \rightarrow \mathbb{R}$ about which we wish to perform inference and a set of input points $\mathbf{x} \subseteq \mathcal{X}$, the Gaussian process prior distribution over the function values $\mathbf{f} = f(\mathbf{x})$ is given by:
\begin{align}
p(\mathbf{f} | \mathbf{x},\bm{\theta},I) &:= \mathcal{N}\left( \mathbf{f};\mu_\theta(\mathbf{x}),K_\theta(\mathbf{x},\mathbf{x}) \right) \nonumber \\
&:= \frac{1}{\sqrt{\mathrm{det} 2 \pi K_{\mathbf{f}}} } \exp \left( - \frac{1}{2} (\mathbf{f}-\mu_{\mathbf{f}})^\top K_\mathbf{f}^{-1} (\mathbf{f}-\mu_{\mathbf{f}})  \right)
\end{align}
where $\bm{\theta}$ is a vector containing any parameters required by $\mu$ and $K$: the \emph{hyperparameters} of the model, $I$. Due to the ubiquity of $I$ we henceforth drop it from explicit representation for notational convenience. %The context, $I$, forms the background knowledge upon which all our probabilities are conditioned. Its ubiquity leads us to henceforth drop it from explicit representation.
There exist a wide variety of mean and covariance functions which can be chosen in order to reflect any prior knowledge available about the function of interest. 
%Note also that we need not place a \verb"GP" directly on the function, for example a function known to be strictly positive might benefit from a \verb"GP" over its logarithm.

%Observations
Once we have observations of the function $(\mathbf{x_s},\yd)$ we can make predictions about the function value $f_*$ at input $x_*$. As exact measurements of the function are often not available we assume a noise model, such that:
\begin{equation}\label{obsnoise}
p(y | f, x, \sigma_n^2) := \mathcal{N}(y; f, \sigma_n^2)
\end{equation} 
which represents i.i.d Gaussian observation noise with variance $\sigma_n^2$. As should be expected, the predictive distribution over $f_*$ is Gaussian:
\begin{equation}\label{posteriorpred}
p(f_* | x_*, \yd,\bm{\theta}) := \mathcal{N} ( f_* ; m_\theta(f_* | x_*, \yd), C_\theta(f_* | x_*, \yd))
\end{equation}
where the posterior mean and covariance are:
\begin{align}
m_\theta(f_* | x_*, \yd) := \mu_\theta(x_*) + K_\theta(x_*,\xd)V^{-1}(\yd - \mu_\theta(x_*))\\
C_\theta(f_* | x_*, \yd) := K_\theta(x_*,x_*) - K_\theta(x_*,\xd)V^{-1}K_\theta(\xd,x_*)
\end{align}
where $V := K_\theta(\xd,\xd) + \sigma_n^2\mathbf{I}$.

\subsection{Marginalising out the Hyperparameters}
%Marginalisation
The previous equations assume that the hyperparameters $\bm{\theta}$ are known; in fact we can rarely be certain about $\bm{\theta}$ \emph{a priori}. This ignorance can be represented by a suitably uninformative prior distribution $p(\bm{\theta})$. Given such a \emph{hyper-prior}, the hyperparameters can be marginalised to calculate the predictive distribution over $f_*$:
\begin{equation}\label{fullmargint}
p(f_* |x_*, \yd) = \frac{\int p(f_* | x_*, \yd,\bm{\theta})p(\yd|\xd,\bm{\theta})p(\bm{\theta})d\bm{\theta}}{\int p(\yd|\xd,\bm{\theta})p(\bm{\theta})d\bm{\theta}}
\end{equation}
Unfortunately, such integrals are generally non-analytic, requiring numerical approximation. Randomized Monte Carlo techniques \cite{chen2000monte} form the most popular approaches to numerical integration, although Bayesian alternatives \cite{bayesquad, osbornebayesquad} also exist. 
These techniques estimate the integral given
the value of the integrand on a set of sample points, usually via a weighted mixture
\begin{equation}\label{sampling}
p(f_* |x_*, \yd) \simeq \sum_i \rho_i p(f_* | x_*, \yd,\bm{\theta}_i)
\end{equation}
for some weight vector $\bm{\rho}$. Unfortunately, the computational expense of evaluating the integrand at sufficient samples to estimate high-dimensional integrals is often prohibitive. As a consequence, such approaches are rarely used for the marginalisation of \verb"GP" hyperparameters. 
%\jansays{IS THIS CORRECT? OR, IS IT BECAUSE IT IS HARD  TO SPECIFY A PRIOR AND SINCE PEOPLE ARE LAZY ?}
% you're not going to win people over by calling them lazy

A less computationally demanding alternative is to select only a single sample. Type II maximum likelihood, or maximum marginal likelihood, approximates as
\begin{equation} \label{eq:ML}
p(f_* |x_*, \yd) \simeq p(f_* | x_*, \yd,\bm{\theta}_{\mathrm{ML}})
\end{equation}
%\begin{equation}\label{marglikelihood}
%p(\yd | \xd, \bm{\theta}) = \int p(\yd | \mathbf{f},\xd, \bm{\theta})p(\mathbf{f} | \mathbf{x},\bm{\theta}) d\mathbf{f}
%\end{equation}
where $\bm{\theta}_{\mathrm{ML}}$ is the hyperparameter vector that maximises the marginal likelihood, 
\begin{equation}
\bm{\theta}_{\mathrm{ML}} = \argmax_{\bm{\theta}} \;\; p(\yd|\xd,\bm{\theta})\,.
\end{equation}
As per Figure ****, \eqref{eq:ML} is equivalent to approximating the likelihood $p(\yd|\xd,\bm{\theta})$ as the delta function $\delta(\theta - \theta_{\mathrm{ML}})$. This assumption is a poor representation of our true state of ignorance given only low numbers of data, for which the likelihood is typically broad and/or multi-modal. As a consequence, type II maximum likelihood can often lead to over-fitting. Nonetheless, the less onerous computational requirements of this approach have made it ubiquitous throughout machine learning. 

We now present a novel means of marginalising hyperparam

%------- APPROXIMATE HYPERPARAMETER MARGINALISATION --------
\section{Approximate Hyperparameter Marginalisation}

In this section we introduce an approximate method for performing inference using a Gaussian process which accommodates the inherent uncertainty one should have about the value of hyperparameters, but which does not suffer from excessive computational expense. Rather than attempt to approximate the integral in (\ref{fullmargint}), we present a philosophical shift away from the standard \verb"GP" prior opting instead to develop a modified prior which incorporates and marginalises the uncertainty in the hyperparameters \emph{before} it is fused with observations. The result is an adjusted covariance function which can be used interchangeably with a standard covariance function in \verb"GP" inference, but whose parameters are now \emph{hyper-hyperparameters} explaining the marginalised covariance function.

\subsection{Homogenous Exponential Covariance Functions} \mikesays{this section defines the general class of covariance functions our method is appropriate to, which we can call Homogenous Exponential covariance functions}

As is the case in standard \verb"GP" inference, a prior covariance function must be selected which is representative of any prior belief about the function of interest. In order to develop our method we first look at one of the most pervasive covariance functions used in Bayesian inference: the \emph{Squared Exponential}, or \emph{Gaussian}, kernel:
\begin{equation}
K_\theta(x,x^\prime) = h^2 \exp \left( -\frac{1}{2} \frac{\bigtriangleup^2}{L^2} \right)
\end{equation}
where $\bigtriangleup^2 = (x - x^\prime)^2$, and $h$ is a scaling parameter termed the output scale. The parameter $L$, the \emph{length scale}, affects how closely points co-vary as a function of $\bigtriangleup^2$. The length parameter is perhaps the most crucial hyperparameter to train correctly, and is where we begin our exposition.

\mikesays{We can also say that marginalising the output scale can be done
analytically with an inverse-gamma prior on the squared output scale,
but then we would no longer have a GP, which would kind of defeat the
point. Alternatively, you can use our approach to approximately
marginalise the output scale h by resolving
$int h^2 K(x, x') p(h) dh$
where $K(x, x')$ is a normalised cov fn. Taking a Gamma prior for $h^-2$,
rather than a Gaussian for  $log h$, is a sensible thing to do here as,
with reference to the previous plots, we'd rather favour infinite
output scale over zero output scale as we get more and more uncertain.
That is, if we have no idea of the output scale at all, it seems
unreasonable for us to expect most of the data to be within epsilon of
the prior mean. With a Gamma prior for $h^-2$ with mean $mu_h$ and shape
parameter $a_h$, our integral resolves to
$a_h Gamma(-1 + a_h) / (mu_h Gamma(a_h) ) K(x, x')$
where Gamma is the Gamma function. So basically we just replace the
squared output scale of our covariance function with $a_h Gamma(-1 +
a_h) / (mu_h Gamma(a_h) )$ and we're done. This trick has been done by previous authors- I can chase up references.}

\jansays{ COULD WE NOT BE MORE GENERAL SAYING:  
We assume our GP prior is endowed with a kernel function of the form 
\begin{equation}
K_\theta(x,x^\prime) = h^2 \exp \left( -\frac{1}{2} \frac{d(x,x')}{L^2} \right)
\end{equation}
where $d$ is an arbitrary function. Typically, $d$ will be a distance metric. For instance, choosing $d(x,x') = x M x'$ for a pos. def. matrix $M$ yields the squared exponential kernel. Or, $d(x,x') = |x-x'|$ gives rise to the Ornstein-Uhlenbeck kernel.
... this should not affect our derivations below..}

\subsection{Rational Quadratic Covariance Function} 

The Rational Quadratic covariance funciton represents one means of approximately marginalising the input scale of a squared exponential covariance. It assumes a gamma prior, which, problematically, puts excessive weight on very lagre input scales when we are very uncertain. See Figures ***** \mikesays{ Mike will make these figures}.

\subsection{Marginal Homogenous Exponential Covariance Function} \mikesays{now we talk about our covariance, which I've called Marginal Homogenous Exponential covariances}


The standard \verb"GP" prior assumes that the value of $L$ is deterministic; any distribution an individual has over $L$ is purely the epistemic characterisation of their uncertainty in the `true' value of $L$. We alter the prior such that we make $L$ a random variable with its own generative distribution $p(L)$. The value of $L$ should be strictly positive \jansays{no the way you defined it above L can be anything. Maybe should def $\exp(-\Delta^2 / L)$ instead of .../$L^2$.. this was the way we used to do it in February}, hence we make the substitution $L=\exp(\beta)$ \jansays{$\beta := \log(L)$ } and define a Gaussian distribution over $\beta$ so that $p(L)$ will have strictly positive support: 
\begin{equation}\label{betaSEkernel}
K_\theta(x,x^\prime) = h^2 \exp \left( -\frac{1}{2}\bigtriangleup^2 \exp(-2\beta) \right)
\end{equation}
which we will write as $K_\beta$, and
\begin{align}
p(\beta|\nu,\Lambda) &= \mathcal{N}(\beta; \nu, \Lambda) \nonumber\\
&= \frac{1}{\sqrt{2 \pi \Lambda}} \exp \left( \frac{ -(\beta - \nu)^2}{2\Lambda} \right) \label{betapdf}
\end{align}
where $\nu$ and $\Lambda$ form hyper-hyperparmeters.

As stated before, we marginalise our the uncertainty in $\beta$ (or $L$), before fusing the prior with observations. 

\jansays{Perhaps we should talk about the moment matching idea first.. As far as I can remember: Ie taylor expand the marginalized process ... it then turns out the the kernel of the matched process equals the marginalized kernel you write down in (14) below..}

The marginalised kernel is calculated by evaluating the following integral:
\begin{equation} \label{kernelmargint}
K_{\nu,\Lambda} = \int_{-\infty}^{+\infty} K_\beta \; p(\beta|\nu,\Lambda) d\beta
\end{equation}
which is non-analytic given the current form of $K_\beta$. We approximate the solution to the integral by first approximating $K_\beta$ using a second-order Taylor expansion around the mean of $p(\beta)$ given by $\nu$. In order to make the integral analytic, we would like $K_\beta$ to be in exponential form, hence we note the following identity and substitution:
\begin{align}
K_\beta &= \exp \left( -\ln \frac{1}{K_\beta} \right) \nonumber\\ 
&= \exp \left( -A \right) \label{Asub}
\end{align}
and we Taylor expand around $A$:
\begin{equation} \label{taylorexpansion}
A \approx \ln \frac{1}{K_{\beta}}\bigg|_{\nu} + (\beta-\nu) \frac{\partial{A}}{\partial{\beta}}\bigg|_{\nu} + \frac{1}{2}(\beta-\nu)^2 \frac{\partial^2{A}}{\partial^2{\beta}}\bigg|_{\nu}
\end{equation}
Given (\ref{betaSEkernel}), the derivatives of $K_\beta$ can be calculated:
\begin{align*}
K^\prime_\beta &= K_\beta \left( \bigtriangleup^2 \exp(-2\beta)  \right) \\ 
K^{\prime\prime}_\beta &= K_\beta \left( \bigtriangleup^4 \exp(-4\beta) -  \bigtriangleup^2 \exp(-2\beta) \right)
\end{align*}
which can be used for calculating the partial derivatives of (\ref{taylorexpansion}):
\begin{align}
A &= \ln \frac{1}{K_\beta} \label{partial1}\\
\frac{\partial{A}}{\partial{\beta}} &= -\frac{K^\prime_\beta}{K_\beta} = \; -\bigtriangleup^2 \exp(-2\beta) \label{partial2}\\
\frac{\partial^2{A}}{\partial^2{\beta}} &= -\frac{K^{\prime\prime}_\beta}{K_\beta} + \frac{{K^\prime_\beta}^2}{K_\beta^2} = \; 2 \bigtriangleup^2 \exp(-2\beta) \label{partial3}
\end{align}
Using (\ref{Asub}) and substituting (\ref{partial1}-\ref{partial3}) into (\ref{taylorexpansion}) we arrive at the following expression for $K_\beta$:
\begin{align}
K_\beta &\approx \exp \left(-\left( \ln \frac{1}{K_{\nu}} - (\beta-\nu) C + (\beta-\nu)^2 C \right)   \right) \nonumber\\
&= K_{\nu} \, \exp \left( (\beta-\nu) C - (\beta-\nu)^2 C \right) \label{approxkernel}
\end{align}
where $C = \bigtriangleup^2 \exp(-2\nu)$

All that remains is to evaluate the integral in (\ref{kernelmargint}), which by using (\ref{betapdf}) and (\ref{approxkernel}) is as follows:
\begin{align}
K_{\nu,\Lambda} &\approx K_{\nu}\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi \Lambda}} \exp \left( \frac{ -(\beta - \nu)^2}{2\Lambda} \right) \left( (\beta-\nu) C - (\beta-\nu)^2 C \right) \nonumber \\
&= K_{\nu} \; \exp \left( \frac{\Lambda\, C^2}{2(1+2\Lambda C)} \right) \frac{1}{\sqrt{1+ 2\Lambda C}}
\end{align}
This result is a new kernel which is the product of the original kernel $K_\beta$ evaluated at $\beta=\nu$, and an expression which is a function of the squared distance $ \bigtriangleup^2$. 



\subsubsection{Proof of Positive Semi-Definiteness}

In order to confirm that our new kernel $K_{\nu,\Lambda}$ is a legitimate covariance function, we must prove that it fulfils the necessary condition of kernels which is positive semi-definiteness. We first note that a sum of kernels is also a kernel, therefore:
\begin{equation}
K_{\nu,\Lambda} = \int K_\beta \;p(\beta)d\beta
\end{equation}
should be a legitimate kernel if $K_\beta$ is also a legitimate kernel as $p(\beta)$ just weights the contents of the integral. Using the expression for $K_\beta$ in (\ref{approxkernel}) and completing the square we arrive at the following expression:
\begin{align}
K_\beta &= K_{\nu} \,  \exp \left(  -\frac{1}{2} \left( \beta - \nu - \frac{1}{2} \right) ^ 2C   \right) \exp \left( \frac{1}{4}C \right)
\end{align}
If we substitute in $K_\nu$, where:
\begin{align}
K_{\nu} &= h^2 \exp\left( -\frac{1}{2}\bigtriangleup^2 \exp(-2\nu) \right) \nonumber\\
&= h^2 \exp\left( -\frac{C}{2} \right)
\end{align}
we can write:
\begin{align}
%K_{\beta} &= h^2 \exp\left( -\frac{C}{2} \right) \,  \exp \left(  -\frac{1}{2} \left( \beta - \nu - \frac{1}{2} \right) ^ 2C   \right) \exp \left( \frac{1}{4}C \right) \nonumber \\
K_{\beta} = h^2  \exp\left( -\frac{C}{4} \right) \,  \exp \left(  -\frac{1}{2} \left( \beta - \nu - \frac{1}{2} \right) ^ 2C   \right) \label{kexpanded}
\end{align}
We note that the product of kernels is also a kernel. Both exponentials in (\ref{kexpanded}) form legitimate kernels, which leaves us with the result that $K_\beta$ is also a kernel and by definition is positive semi-definite as required.






%------- EXPERIMENTS --------
\section{Experiments}




%------- RELATED WORK --------
%\section{Related Work} 
% \mjikesays{no need for this particularly, I don't think; we can cover related work above}




%------- CONCLUSION --------
\section{Conclusion}



%---------ACKNOWLEDGEMENTS-----------
% \subsubsection*{Acknowledgments}
% Do we have any? Aladdin / Orchid?
% no acknowledgements in double-blind submission


%------------REFERENCES----------------
\subsubsection*{References}
\renewcommand{\refname}{\vskip -0.75cm}  %Removes the "References" title
\bibliographystyle{plain}
\small{
\bibliography{Hypermarginalisation}
}




\end{document}






