\documentclass{article}
\usepackage{nips12submit_e,times}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}

\newcommand{\xd}{\mathbf{x}}
\newcommand{\yd}{\mathbf{y}}

\DeclareMathOperator*{\argmax}{\arg\!\max\!}

\title{Approximate Hyperparameter Marginalisation for Gaussian Processes}

\author{
Rob\\
\And
Steve\\
\And
Jan\\
\And
Mike\\
\And
Steve\\}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version


\begin{document}


\maketitle

%------- ABSTRACT --------
\begin{abstract}
Enter Abstract here
\end{abstract}


%------- INTRODUCTION--------
\section{Introduction}



%------- GAUSSIAN PROCESSES--------
\section{Gaussian Processes}

Gaussian processes (\verb"GPs") constitute a powerful method for performing Bayesian inference about functions using a limited set of observations \cite{rassandwill}. A \verb"GP" is defined as a distribution over the functions $f : \mathcal{X} \rightarrow \mathbb{R}$ such that the distribution over the possible function values on any finite set of $\mathcal{X}$ is multi-variate Gaussian. A vector of observations $\mathbf{y} = \{ y_1,...,y_n\}$ could be viewed as a single point sampled from a $n$-variate Gaussian distribution.
%One can imagine a point $x_i \in \mathcal{X}$ as having an associated random variable $Y_i$ representing the possible values the function can take at that point, the \verb"GP" is therefore the infinite set of such random variables defined across the domain of the function. If we take a sample from a \verb"GP" at a finite set of input locations $\mathbf{x} = \{x_1,...,x_n\}$, the corresponding observations $\mathbf{y} = \{ y_1,...,y_n\}$ can be viewed as a single point sampled from some multi-variate Gaussian distribution.

A \verb"GP" is completely defined by its first and second moments: a mean function $\mu : \mathcal{X} \rightarrow \mathbb{R}$, which describes the overall trend of the function, and a positive semidefinite covariance function $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ which describes how function values are correlated as a function of their locations in the domain. Given a function $f : \mathcal{X} \rightarrow \mathbb{R}$ about which we wish to perform inference and a set of input points $\mathbf{x} \subseteq \mathcal{X}$, the Gaussian process prior distribution over the function values $\mathbf{f} = f(\mathbf{x})$ is given by:
\begin{align}
p(\mathbf{f} | \mathbf{x},\bm{\theta},I) &:= \mathcal{N}\left( \mathbf{f};\mu_\theta(\mathbf{x}),K_\theta(\mathbf{x},\mathbf{x}) \right) \\
&:= \frac{1}{\sqrt{\mathrm{det} 2 \pi K_{\mathbf{f}}} } \exp \left( - \frac{1}{2} (\mathbf{f}-\mu_{\mathbf{f}})^\top K_\mathbf{f}^{-1} (\mathbf{f}-\mu_{\mathbf{f}})  \right)
\end{align}
where $\bm{\theta}$ is a vector containing any parameters required by $\mu$ and $K$: the \emph{hyperparameters} of the model, $I$. Due to the ubiquity of $I$ we henceforth drop it from explicit representation for notational convenience. %The context, $I$, forms the background knowledge upon which all our probabilities are conditioned. Its ubiquity leads us to henceforth drop it from explicit representation.
There exist a wide variety of mean and covariance functions which can be chosen in order to reflect any prior knowledge available about the function of interest. 
%Note also that we need not place a \verb"GP" directly on the function, for example a function known to be strictly positive might benefit from a \verb"GP" over its logarithm.

%Observations
Once we have observations of the function $(\mathbf{x_s},\yd)$ we can make predictions about the function value $f_*$ at input $x_*$. As exact measurements of the function are often not available we assume a noise model, such that:
\begin{equation}\label{obsnoise}
p(y | f, x, \sigma_n^2) := \mathcal{N}(y; f, \sigma_n^2)
\end{equation} 
which represents i.i.d Gaussian observation noise with variance $\sigma_n^2$. As should be expected, the predictive distribution over $f_*$ is Gaussian:
\begin{equation}\label{posteriorpred}
p(f_* | x_*, \yd,\bm{\theta}) := \mathcal{N} ( f_* ; m_\theta(f_* | x_*, \yd), C_\theta(f_* | x_*, \yd))
\end{equation}
where the posterior mean and covariance are:
\begin{align}
m_\theta(f_* | x_*, \yd) := \mu_\theta(x_*) + K_\theta(x_*,\xd)V^{-1}(\yd - \mu_\theta(x_*))\\
C_\theta(f_* | x_*, \yd) := K_\theta(x_*,x_*) - K_\theta(x_*,\xd)V^{-1}K_\theta(\xd,x_*)\\
\mathrm{where}\quad V := K_\theta(\xd,\xd) + \sigma_n^2\mathbf{I}
\end{align}

\subsection{Marginalising out the Hyperparameters}
%Marginalisation
The previous equations assume that the hyperparameters $\bm{\theta}$ are known; in fact we can rarely be certain about $\bm{\theta}$ \emph{a priori}. This ignorance can be represented by a suitably uninformative prior distribution $p(\bm{\theta})$. Given such a \emph{hyper-prior}, the hyperparameters can be marginalised to calculate the predictive distribution over $f_*$:
\begin{equation}\label{fullmargint}
p(f_* |x_*, \yd) = \frac{\int p(f_* | x_*, \yd,\bm{\theta})p(\yd|\xd,\bm{\theta})p(\bm{\theta})d\bm{\theta}}{\int p(\yd|\xd,\bm{\theta})p(\bm{\theta})d\bm{\theta}}
\end{equation}
Unfortunately, such integrals are generally non-analytic, requiring numerical approximation. Randomized Monte Carlo techniques \citet{chen2000monte} form the most popular approaches to numerical integration, although Bayesian alternatives \cite{bayesquad, osborne_aistats} also exist. 
These techniques estimate the integral given
the value of the integrand on a set of sample points, usually via a weighted mixture
\begin{equation}\label{sampling}
p(f_* |x_*, \yd) \simeq \sum_i \rho_i p(f_* | x_*, \yd,\bm{\theta}_i)
\end{equation}
for some weight vector $\bm{\rho}$. Unfortunately, the computational expense of evaluating the integrand at sufficient samples to estimate high-dimensional integrals is often prohibitive. As a consequence, such approaches are rarely used for the marginalisation of \verb"GP" hyperparameters. 

A less computationally demanding alternative is to select only a single sample. Type II maximum likelihood, or maximum marginal likelihood, approximates as
\begin{equation} \label{eq:ML}
p(f_* |x_*, \yd) \simeq p(f_* | x_*, \yd,\bm{\theta}_{\mathrm{ML}})
\end{equation}
%\begin{equation}\label{marglikelihood}
%p(\yd | \xd, \bm{\theta}) = \int p(\yd | \mathbf{f},\xd, \bm{\theta})p(\mathbf{f} | \mathbf{x},\bm{\theta}) d\mathbf{f}
%\end{equation}
where $\bm{\theta}_{\mathrm{ML}}$ is the hyperparameter vector that maximises the marginal likelihood, 
\begin{equation}
\bm{\theta}_{\mathrm{ML}} = \argmax_{\bm{\theta}} \;\; p(\yd|\xd,\bm{\theta})\,.
\end{equation}
As per Figure ****, \eqref{eq:ML} is equivalent to approximating the likelihood $p(\yd|\xd,\bm{\theta})$ as the delta function $\delta(\theta - \theta_{\mathrm{ML}})$. This assumption is a poor representation of our true state of ignorance given only low numbers of data, for which the likelihood is typically broad and/or multi-modal. As a consequence, type II maximum likelihood can often lead to over-fitting. Nonetheless, the less onerous computational requirements of this approach have made it ubiquitous throughout machine learning. 

*+*+*+*+*+*+*+ TEXT NEEDED!!! - Insert comment about what our paper does as a gap between the two methods just discussed *+*+*+*+*+*+*+*+

%------- APPROXIMATE HYPERPARAMETER MARGINALISATION --------
\section{Approximate Hyperparameter Marginalisation}

$L = \exp(\beta)$

\begin{align}
p(\beta|\nu,\Lambda) &= \mathcal{N}(\beta; \nu, \Lambda)\\
&= \frac{1}{\sqrt{2 \pi \Lambda}} \exp \left( \frac{ -(\beta - \nu)^2}{2\Lambda} \right)
\end{align}

\begin{align}
K_\beta &= \exp \left( -\ln \frac{1}{K_\beta} \right)\\ 
&= \exp \left( -A \right)
\end{align}


\begin{equation} \label{taylorexpansion}
A \approx \ln \frac{1}{K_{\beta}}\bigg|_{\nu} + (\beta-\nu) \frac{\partial{A}}{\partial{\beta}}\bigg|_{\nu} + \frac{1}{2}(\beta-\nu)^2 \frac{\partial^2{A}}{\partial^2{\beta}}\bigg|_{\nu}
\end{equation}

\begin{align}%Probably use this earlier
K_\beta &= h^2 \exp\left( -\frac{1}{2}\bigtriangleup^2 \exp(-2\beta) \right)\\
K^\prime_\beta &= K_\beta \left( \bigtriangleup^2 \exp(-2\beta)  \right) \\
K^{\prime\prime}_\beta &= K_\beta \left( \bigtriangleup^4 \exp(-4\beta) -  \bigtriangleup^2 \exp(-2\beta) \right)
\end{align}

\begin{align}
A &= \ln \frac{1}{K_\beta} \\
\frac{\partial{A}}{\partial{\beta}} &= -\frac{K^\prime_\beta}{K_\beta} = \; \bigtriangleup^2 \exp(-2\beta)\\
\frac{\partial^2{A}}{\partial^2{\beta}} &= -\frac{K^{\prime\prime}_\beta}{K_\beta} + \frac{{K^\prime_\beta}^2}{K_\beta^2} = \; 2 \bigtriangleup^2 \exp(-2\beta)
\end{align}

$C = \bigtriangleup^2 \exp(-2\nu)$

%... from Taylor Expansion
\begin{align}
K_\beta &\approx \exp \left(-\left( \ln \frac{1}{K_{\nu}} + (\beta-\nu) C + (\beta-\nu)^2 C \right)   \right)\\
&= K_{\nu} \, \exp \left( (\beta-\nu) C + (\beta-\nu)^2 C \right)
\end{align}

%Marginalising Beta
\begin{align}
K_{\nu,\Lambda} &= \int_{-\infty}^{+\infty} K_\beta \; p(\beta|\nu,\Lambda) d\beta \\
&= K_{\nu}\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi \Lambda}} \exp \left( \frac{ -(\beta - \nu)^2}{2\Lambda} \right) \left( (\beta-\nu) C + (\beta-\nu)^2 C \right) \\
&= K_{\nu} \; \exp \left( \frac{\Lambda\, C^2}{2(1+2\Lambda C)} \right) \frac{1}{\sqrt{1+ 2\Lambda C}}
\end{align}








\subsection{Proof of Positive Semi-Definiteness}

*+*+*+*+ I'll tidy this up!

A sum of kernel is itself a kernel, which by definition fulfils the necessary condition of positive semi-definiteness. Therefore:
\begin{equation}
K_{\nu,\Lambda} = \int K_\beta \;p(\beta)d\beta
\end{equation}
should be a legitimate kernel if $K_\beta$ is also a legitimate kernel as $p(\beta)$ just weights the contents of the integral.

%From above SAME EQUATION!!!! (SO REMOVE LATER)
\begin{equation}
K_\beta = K_{\nu} \, \exp \left( (\beta-\nu) C + (\beta-\nu)^2 C \right)
\end{equation}
completing the square:
\begin{align}
K_\beta &= K_{\nu} \,  \exp \left(  -\frac{1}{2} (\beta - \nu - 1) ^ 2C   \right) \exp \left( \frac{1}{2}C \right) \\
\end{align}
Product of kernels is also a kernel. Therefore if all three parts of the above equation are kernel, then $K_\beta$ is also a covariance function. First two parts are kernels, the last part isn't. However:
\begin{align}
K_{\nu} &= h^2 \exp\left( -\frac{1}{2}\bigtriangleup^2 \exp(-2\nu) \right)\\
&= h^2 \exp\left( -\frac{C}{2} \right)\\
K_{\nu} \exp \left( \frac{1}{2}C \right) &= h^2 \exp\left( -\frac{C}{2} \right) \exp \left( \frac{1}{2}C \right) \\
&= h^2 \exp\left( 0 \right)
\end{align}
so $K_\beta$ is a kernel.

*+*+*+*+ I need to check this, as the result has changed since I did my substitution...





%------- EXPERIMENTS --------
\section{Experiments}




%------- RELATED WORK --------
\section{Related Work}




%------- CONCLUSION --------
\section{Conclusion}



%---------ACKNOWLEDGEMENTS-----------
\subsubsection*{Acknowledgments}
Do we have any? Aladdin / Orchid?


%------------REFERENCES----------------
\subsubsection*{References}
\renewcommand{\refname}{\vskip -0.75cm}  %Removes the "References" title
\bibliographystyle{plain}
\small{
\bibliography{Hypermarginalisation}
}




\end{document}






